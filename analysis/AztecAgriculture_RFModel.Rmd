---
title: "Aztec Agricultural Productivity Model"
subtitle: "Part 3: Random Forest Model"
author: "Rudolf Cesaretti"
date: "Last run on `r Sys.Date()`"
output:
  html_document:
    toc: true
    df_print: paged
    number_sections: true
bibliography: References.bib
csl: apa.csl
link-citations: yes
---

```{css, echo=FALSE}
pre {
  max-height: 300px;
  overflow-y: auto;
}

pre[class] {
  max-height: 300px;
}
```

```{r, setup, include=FALSE,echo=FALSE, message=FALSE,warning=FALSE}
require(knitr)
# Set so that long lines in R will be wrapped:
opts_chunk$set(tidy.opts=list(width.cutoff=75),tidy=TRUE)
#
rm(list = ls())
```

I do four things in this R markdown document: 
Topographic/environmental metrics

  1. Calculate catchment area variables, including
      + Topographic/environmental metrics
      + 
  4. Reorganize the data and export for Script #6
  
  
# Setup 

All of the data and scripts are downloadable from the [new ASU SettlementPersist2022 github repository](https://https://github.com/rcesaret/ASUSettlementPersist2022), which can be downloaded locally as a .zip folder or cloned to your own account.

Either way, once you have done so, you will need to modify the working directory (setwd("C:/...)") path and "dir" variables in the code chunk below to match the repository location on your computer.

```{r, label='Set Local Directory Location', message=FALSE,warning=FALSE}

wd <- list()

#SET YOUR LOCAL DIRECTORY LOCATION HERE:
wd$dir <- "D:/Dropbox (ASU)/AztecAgricultureModel/AztecAgriculture/"
#wd$dir <- "C:/Users/TJ McMote/Dropbox (ASU)/AztecAgricultureModel/AztecAgriculture/"

wd$analysis <- paste0(wd$dir,"analysis/")
wd$data_r <- paste0(wd$dir,"data-raw/")
wd$data_p <- paste0(wd$dir,"data-processed/")
wd$data_f <- paste0(wd$dir,"data-final-outputs/")
wd$figs <- paste0(wd$dir,"figures/")
wd$funcs <- paste0(wd$dir,"functions/")

```



## Load R Packages and Custom Functions

```{r, label='Load Libraries', message=FALSE,warning=FALSE}
# Package names
packages <- c("tidyverse", "rgdal", "rgeos", "sp", "sf", "GISTools", "raster", 
              "Matrix", "terra","gdistance", "lwgeom", "tidyr", "stars", 
              "dismo", "purrr", "spatialEco", "whitebox", "classInt",
              "ggnewscale", "lbmech", "data.table", "tidyterra","gridExtra", 
              "cowplot", "scam", "rmarkdown", "spatialreg","spdep", "ggridges", 
              "ggnewscale", "scales", "ggstatsplot", "stringi", "fuzzyjoin", 
              "mgcv", "randomForest", "ranger", "exactextractr", "kableExtra")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# load packages
invisible(lapply(packages, library, character.only = TRUE))

rm(packages,installed_packages)

#Read in custom R functions located in the wd$funcs directory folder
FUNCS <- list("RescaleSpatRast.R", "Terra_df.R", "RF_ImputeRast.R")
invisible(lapply(FUNCS, function(x) source(paste0(wd$funcs,x))))
rm(FUNCS)

```




## Load Data from Previous Script

```{r, label='Import Data from Part 1', message=FALSE,warning=FALSE}
Municipios2000s <- st_read(paste0(wd$data_p, "Municipios2000s_Data.gpkg"))

Data2000s_poly <- st_read(paste0(wd$data_p, "Data2000s_poly.gpkg"))

TopoEnv_r <- rast(paste0(wd$data_p, "TopoEnvData.tif"))

Soil_r <- rast(paste0(wd$data_p, "Soil_r_resampled.tif"))

Clim_r <- rast(paste0(wd$data_p, "Clim_r_resampled.tif"))

```




# Construct Training Dataset

## Environmental Raster Data

### Subset Raster Data

```{r, label='Subset Environmental Data Rasters', message=FALSE,warning=FALSE}
Soil_r_sub <- subset(Soil_r, c("BD","S", "Z","C","SOC","CEC","OCD","N", "BDRICM", "BDTICM"))

TopoEnv_r_sub <- subset(TopoEnv_r, c("DEM", "Slope","Accum", "TRI", "TWI", "STI", "SPI"))

Clim_r_sub <- subset(Clim_r, c("prec_04", "prec_05", "prec_06", "prec_07", "prec_08", "prec_09", "prec_10", "prec_11",
                               "pr_04", "pr_05", "pr_06", "pr_07", "pr_08", "pr_09", "pr_10", "pr_11",
                               "tmean_04", "tmean_05", "tmean_06", "tmean_07", "tmean_08", "tmean_09", "tmean_10", "tmean_11", 
                               "tmin_04","tmin_05", "tmin_06", "tmin_07", "tmin_08", "tmin_09", "tmin_10","tmin_11",
                               "clt_04", "clt_05", "clt_06", "clt_07", "clt_08", "clt_09", "clt_10", "clt_11", 
                               "rsds_04", "rsds_05", "rsds_06", "rsds_07", "rsds_08", "rsds_09", "rsds_10", "rsds_11"
                               ))

```


When aggregating average values of individual months into single values for a range of months, you have several options depending on your specific requirements and the nature of the data. Here are a few approaches commonly used in statistical modeling:

Summation: Add up the values for the individual months within the range to obtain a single value. This approach is appropriate when the variable you are aggregating is additive in nature, such as precipitation or degree days. For example, if you want to aggregate May through July, you would sum the values for May, June, and July.

Averaging: Calculate the average of the values for the individual months within the range. This approach works well when the variable you are aggregating represents a continuous process, such as temperature. For example, if you want to aggregate May through July, you would calculate the average of the values for May, June, and July.

Maximum/Minimum: Use the maximum or minimum value among the individual months within the range. This approach is suitable when you want to capture the extreme values or the peak of a certain variable over a period. For instance, if you want to aggregate May through July, you would select the highest or lowest value among May, June, and July.

Weighted Average: Assign weights to the individual months based on their importance or contribution to the overall season and calculate the weighted average. This approach allows you to give more emphasis to certain months within the range. For example, if you believe June is more critical to the growing season than May or July, you can assign a higher weight to June in the calculation of the weighted average.

The choice of aggregation method depends on the specific characteristics of your data and the particular research question you are investigating. Consider the temporal dynamics, significance of individual months, and the underlying relationship between the climate variables and the agricultural growing season when selecting the most appropriate aggregation method. Additionally, it is advisable to document and justify the chosen method in your research to ensure transparency and reproducibility.

### Extract Raster Values

```{r, label='Extract Raster Values', message=FALSE,warning=FALSE}

topo_vals <- exact_extract(x = TopoEnv_r_sub, y = Data2000s_poly, fun = "mean", 
                           max_cells_in_memory = 8e+08, stack_apply=T, 
                           append_cols = c("EstadoMunicipio","Estado", "Municipio", "AGType"))

soil_vals <- exact_extract(x = Soil_r_sub, y = Data2000s_poly, fun = "mean", 
                           max_cells_in_memory = 8e+08, stack_apply=T, 
                           append_cols = c("EstadoMunicipio","Estado", "Municipio", "AGType"))

clim_vals <- exact_extract(x = Clim_r_sub, y = Data2000s_poly, fun = "mean", 
                           max_cells_in_memory = 8e+08, stack_apply=T, 
                           append_cols = c("EstadoMunicipio","Estado", "Municipio", "AGType"))

env_vals <- topo_vals %>% left_join(soil_vals, by = c("EstadoMunicipio","Estado", "Municipio", "AGType")) %>% 
                          left_join(clim_vals, by = c("EstadoMunicipio","Estado", "Municipio", "AGType")) %>% 
                          mutate(AGType = as.factor(AGType))
  

#env_vals = merge(topo_vals, soil_vals, all = F, sort = F)

rm(topo_vals, soil_vals, clim_vals)

```

```{r}
#values2 <- exact_extract(x = Soil_r_sub, y = Data2000s_poly, fun = "count", max_cells_in_memory = 8e+08, stack_apply=T, append_cols = c("EstadoMunicipio","Estado", "Municipio", "AGType"))

#xx = cbind(st_drop_geometry(Data2000s_poly$EstadoMunicipio), values)

#values <- exact_extract(x = Soil_r_sub[[1:2]], y = Data2000s_poly, fun = mymean, summarize_df = TRUE, max_cells_in_memory = 8e+08, stack_apply=T)

all <- c(Clim_r_sub, Soil_r_sub, TopoEnv_r_sub)

Soil_df <- exact_extract(x = Soil_r_sub, y = Data2000s_poly, fun = mymean, max_cells_in_memory = 8e+10)
save(all_df, file = paste0(wd$data_r, "EnvRastPolyExtract_df.rData"))


st_extract(Clim_r[,,,5], x, fun = mean)

Clim_df <- data.frame()

for (i in 1:nlyr(Clim_r)) {
  out[, i] <- my_function(my_df[, i])
}

nlyr(Clim_r)
values <- exact_extract(x = Clim_r[[1:2]], y = x, fun = "mean", max_cells_in_memory = 8e+08)

c("prec_05", "prec_06", "prec_07", "prec_08", "prec_09", "prec_10", 
"pr_05", "pr_06", "pr_07", "pr_08", "pr_09", "pr_10", 
"tmean_05", "tmean_06", "tmean_07", "tmean_08", "tmean_09", "tmean_10", 
"tmin_05", "tmin_06", "tmin_07", "tmin_08", "tmin_09", "tmin_10")

clim <- stack(prec[[5:7]],prec[[5:7]],
prec[[8:10]],
tmin[[5:7]],
tmin[[8:10]],tmin[[8:10]])
geol <- stack(depth, cation, slope)

geol_vals <- extract(geol,points)
clim_vals <- extract(clim,points)
```




### DELETE Rescale/Normalize Rasters

```{r, label='Rescale Rasters', message=FALSE,warning=FALSE, eval = FALSE}

rescale <- function(z,stdev=2) {
z[z < -stdev] <- -stdev
z[z > stdev] <- stdev
z <- (z + stdev) / (stdev*2)
}

# Subset the variables/layers you want


Soil_r_sub_z = Soil_r_sub
Soil_r_sub_zrs = Soil_r_sub
for (i in 1:nlyr(Soil_r_sub)) {
  Soil_r_sub_z[[i]] <- raster.Zscore(Soil_r_sub[[i]])
  Soil_r_sub_zrs[[i]] <- rescale(Soil_r_sub_z[[i]])
}
writeRaster(Soil_r_sub_z, filename = paste0(wd$data_r, "ll.tif"),  overwrite=TRUE)
writeRaster(Soil_r_sub_zrs, filename = paste0(wd$data_r, "Soil_r_sub_zrs.tif"), overwrite=TRUE)





TopoEnv_r_sub_z = TopoEnv_r_sub
TopoEnv_r_sub_zrs = TopoEnv_r_sub
for (i in 1:nlyr(TopoEnv_r_sub)) {
  TopoEnv_r_sub_z[[i]] <- raster.Zscore(TopoEnv_r_sub[[i]])
  TopoEnv_r_sub_zrs[[i]] <- rescale(TopoEnv_r_sub_z[[i]])
}
writeRaster(TopoEnv_r_sub_z, filename = paste0(wd$data_r, "TopoEnv_r_sub_z.tif"), overwrite=TRUE)
writeRaster(TopoEnv_r_sub_zrs, filename = paste0(wd$data_r, "TopoEnv_r_sub_zrs.tif"), overwrite=TRUE)


prec <- subset(Clim_r, c("prec_04", "prec_05", "prec_06", "prec_07", "prec_08", "prec_09", "prec_10", "prec_11"))

prec_z = prec
prec_zrs = prec
for (i in 1:nlyr(prec)) {
  prec_z[[i]] <- raster.Zscore(prec[[i]])
}
writeRaster(prec_z, filename = paste0(wd$data_r, "prec_z.tif"), overwrite=TRUE)
prec_z <- rast(paste0(wd$data_r, "prec_z.tif"))
for (i in 1:nlyr(prec_z)) {
  prec_zrs[[i]] <- rescale(prec_z[[i]])
}

pr <- subset(Clim_r, c("pr_04", "pr_05", "pr_06", "pr_07", "pr_08", "pr_09", "pr_10", "pr_11"))
pr_z = pr
pr_zrs = pr
for (i in 1:nlyr(pr)) {
  pr_z[[i]] <- raster.Zscore(pr[[i]])
}
for (i in 1:nlyr(prec_z)) {
  pr_zrs[[i]] <- rescale(pr_z[[i]])
}

tmean <- subset(Clim_r, c("tmean_04", "tmean_05", "tmean_06", "tmean_07", "tmean_08", "tmean_09", "tmean_10", "tmean_11"))
tmean_z = tmean
tmean_zrs = tmean
for (i in 1:nlyr(tmean)) {
  tmean_z[[i]] <- raster.Zscore(tmean[[i]])
}
for (i in 1:nlyr(tmeanec_z)) {
  tmean_zrs[[i]] <- rescale(tmean_z[[i]])
}

tmin <- subset(Clim_r, c("tmin_04","tmin_05", "tmin_06", "tmin_07", "tmin_08", "tmin_09", "tmin_10","tmin_11"))
tmin_z = tmin
tmin_zrs = tmin
for (i in 1:nlyr(tmin)) {
  tmin_z[[i]] <- raster.Zscore(tmin[[i]])
}
for (i in 1:nlyr(tminec_z)) {
  tmin_zrs[[i]] <- rescale(tmin_z[[i]])
}



Clim_r_sub <- subset(Clim_r, c("prec_04", "prec_05", "prec_06", "prec_07", "prec_08", "prec_09", "prec_10", "prec_11",
                               "pr_04", "pr_05", "pr_06", "pr_07", "pr_08", "pr_09", "pr_10", "pr_11",
                               "tmean_04", "tmean_05", "tmean_06", "tmean_07", "tmean_08", "tmean_09", "tmean_10", "tmean_11", 
                               "tmin_04","tmin_05", "tmin_06", "tmin_07", "tmin_08", "tmin_09", "tmin_10","tmin_11"#,
                               "clt_04", "clt_05", "clt_06", "clt_07", "clt_08", "clt_09", "clt_10", "clt_11", 
                               "rsds_04", "rsds_05", "rsds_06", "rsds_07", "rsds_08", "rsds_09", "rsds_10", "rsds_11"
                               ))
#Clim_r_sub_z = Clim_r_sub
#Clim_r_sub_zrs = Clim_r_sub
#for (i in 1:nlyr(Clim_r_sub)) {
#  Clim_r_sub_z[[i]] <- raster.Zscore(Clim_r_sub[[i]])
#  Clim_r_sub_zrs[[i]] <- rescale(Clim_r_sub_z[[i]])
#}

writeRaster(Clim_r_sub_z, filename = paste0(wd$data_r, "Clim_r_sub_z.tif"), overwrite=TRUE)
writeRaster(Clim_r_sub_zrs, filename = paste0(wd$data_r, "Clim_r_sub_zrs.tif"), overwrite=TRUE)




# Merge subsetted environmental rasters
all <- c(Clim_r_sub, Soil_r_sub, TopoEnv_r_sub)
all_z <- c(Clim_r_sub_z, Soil_r_sub_z, TopoEnv_r_sub_z)
all_zrs <- c(Clim_r_sub_zrs, Soil_r_sub_zrs, TopoEnv_r_sub_zrs)



all_df <- exact_extract(x = all, y = Data2000s_poly, fun = "mean", max_cells_in_memory = 8e+10)
save(all_df, file = paste0(wd$data_r, "EnvRastPolyExtract_df.rData"))

all_z_df <- exact_extract(x = all_z, y = Data2000s_poly, fun = "mean", max_cells_in_memory = 8e+10)
save(all_z_df, file = paste0(wd$data_r, "EnvRastPolyExtract_z_df.rData"))

all_zrs_df <- exact_extract(x = all_zrs, y = Data2000s_poly, fun = "mean", max_cells_in_memory = 8e+10)
save(all_zrs_df, file = paste0(wd$data_r, "EnvRastPolyExtract_zrs_df.rData"))


```

Random Forest algorithm is not a distance-based model - it is a tree-based model. Each node in a Random Forest is not comparing feature values, it is simply splitting a sorted list that requires absolute values for branching. The algorithm is based on partitioning the data to make predictions, therefore, it does not require normalization.

https://www.kdnuggets.com/2022/07/random-forest-algorithm-need-normalization.html#:~:text=Each%20node%20in%20a%20Random,it%20does%20not%20require%20normalization.



## Agricultural Data


```{r, label='Prepare Agricultural Data', message=FALSE,warning=FALSE}

Data2000s_poly <- Data2000s_poly %>% mutate(
  AGType = as.factor(AGType),
  AvgYield = as.numeric(AvgYield),
  sdYield = as.numeric(sdYield),
  minYield = as.numeric(minYield),
  maxYield = as.numeric(maxYield),
  medYield = as.numeric(medYield),
  cvYield = as.numeric(cvYield),
  
  AvgYieldLoss = as.numeric(AvgYieldLoss),
  sdYieldLoss = as.numeric(sdYieldLoss),
  minYieldLoss = as.numeric(minYieldLoss),
  maxYieldLoss = as.numeric(maxYieldLoss),
  medYieldLoss = as.numeric(medYieldLoss),
  cvYieldLoss = as.numeric(cvYieldLoss),
  
  AvgPlanted = as.numeric(AvgPlanted),
  AvgHarvested = as.numeric(AvgHarvested),
  sdHarvested = as.numeric(sdHarvested),
  cvHarvested = as.numeric(cvHarvested),
  AvgLost = as.numeric(AvgLost),
  sdLost = as.numeric(sdLost),
  cvLost = as.numeric(cvLost),
  AvgProduct = as.numeric(AvgProduct),
  AvgValue = as.numeric(AvgValue),
  AvgPrice = as.numeric(AvgPrice),
  n = as.numeric(n),
  n_Total = as.numeric(n_Total),
  AvgYield_Total = as.numeric(AvgYield_Total),
  sdYield_Total = as.numeric(sdYield_Total),
  cvYield_Total = as.numeric(cvYield_Total),
  AvgYield_Irrig = as.numeric(AvgYield_Irrig),
  sdYield_Irrig = as.numeric(sdYield_Irrig),
  cvYield_Irrig = as.numeric(cvYield_Irrig),
  AvgYield_Temp = as.numeric(AvgYield_Temp),
  sdYield_Temp = as.numeric(sdYield_Temp),
  cvYield_Temp = as.numeric(cvYield_Temp),
  IT_ratio_AvgYield = as.numeric(IT_ratio_AvgYield),
  IT_ratio_cvYield = as.numeric(IT_ratio_cvYield),
  
  AvgYieldLoss_Total = as.numeric(AvgYieldLoss_Total),
  sdYieldLoss_Total = as.numeric(sdYieldLoss_Total),
  cvYieldLoss_Total = as.numeric(cvYieldLoss_Total),
  AvgYieldLoss_Irrig = as.numeric(AvgYieldLoss_Irrig),
  sdYieldLoss_Irrig = as.numeric(sdYieldLoss_Irrig),
  cvYieldLoss_Irrig = as.numeric(cvYieldLoss_Irrig),
  AvgYieldLoss_Temp = as.numeric(AvgYieldLoss_Temp),
  sdYieldLoss_Temp = as.numeric(sdYieldLoss_Temp),
  cvYieldLoss_Temp = as.numeric(cvYieldLoss_Temp),
  IT_ratio_AvgYieldLoss = as.numeric(IT_ratio_AvgYieldLoss),
  IT_ratio_cvYieldLoss = as.numeric(IT_ratio_cvYieldLoss),
  
  Pct_Tract_Mechaniz_pu = as.numeric(Pct_Tract_Mechaniz_pu),
  Pct_Tract_Animal_pu = as.numeric(Pct_Tract_Animal_pu),
  Pct_Tract_Manual_pu = as.numeric(Pct_Tract_Manual_pu),
  Riego_PU = as.numeric(Riego_PU),
  Temporal_PU = as.numeric(Temporal_PU)
  )

```


```{r, label='Prepare Agricultural Data', message=FALSE,warning=FALSE}

x = st_drop_geometry(Data2000s_poly)# %>% filter(-1:3)
x = x %>% filter(!(EstadoMunicipio %in% c("Ixtapaluca, Mexico", "Valle de Chalco Solidaridad, Mexico", "Milpa Alta, Distrito Federal", "Tlahuac, Distrito Federal", "Xochimilco, Distrito Federal", "Axapusco, Mexico") & AGType %in% "Riego"))

#x = x[,-c(1:5)]
#x = as.data.frame(x)
Tot = x %>% select(-AvgYield_Total, -sdYield_Total, -cvYield_Total, 
                   -AvgYield_Irrig, -sdYield_Irrig, -cvYield_Irrig, -AvgYield_Temp, 
                   -sdYield_Temp, -cvYield_Temp, -IT_ratio_AvgYield, -IT_ratio_cvYield, 
                   -AvgYieldLoss_Total, -sdYieldLoss_Total, -cvYieldLoss_Total, 
                   -AvgYieldLoss_Irrig, -sdYieldLoss_Irrig, -cvYieldLoss_Irrig, 
                   -AvgYieldLoss_Temp, -sdYieldLoss_Temp, -cvYieldLoss_Temp, 
                   -IT_ratio_AvgYieldLoss,  -IT_ratio_cvYieldLoss, -minYieldLoss) %>% 
  filter(complete.cases(.))


# Find rows with NA, NaN, or Inf values
#rows_with_na <- apply(Tot, 1, function(row) any(is.na(row) | is.nan(row) | is.infinite(row)))

# Find columns with NA, NaN, or Inf values
#columns_with_na <- apply(Tot, 2, function(column) any(is.na(column) | is.nan(column) | is.infinite(column)))

# Print the rows and columns with NA, NaN, or Inf values
#cat("Rows with NA, NaN, or Inf values:\n")
#print(which(rows_with_na))
#cat("\n")
#cat("Columns with NA, NaN, or Inf values:\n")
#print(which(columns_with_na))




#####Tot = Tot %>% select(-AGType)
#y = as.matrix(y)
#colSums(is.na(y) | is.infinite(y))
#missing_rows <- which(rowSums(is.na(y) | is.infinite(y)) > 0)
#missing_cols <- which(colSums(is.na(y) | is.infinite(y)) > 0)
#cat("Columns with missing or infinite values:", missing_cols, "\n")

```
--n = 1 case--
***- Riego
Malinalco, Mexico 
Chicoloapan, Mexico
Chimalhuacan, Mexico

sdYield     cvYield sdYieldLoss cvYieldLoss sdHarvested cvHarvested      sdLost      cvLost 


***- Riego
Ixtapaluca, Mexico 
Valle de Chalco Solidaridad, Mexico
Milpa Alta, Distrito Federal
Tlahuac, Distrito Federal
Xochimilco, Distrito Federal
Axapusco, Mexico




-AvgYieldLoss_Irrig, -sdYieldLoss_Irrig, -cvYieldLoss_Irrig, -AvgYieldLoss_Temp, -sdYieldLoss_Temp, -cvYieldLoss_Temp, -IT_ratio_AvgYieldLoss,  -IT_ratio_cvYieldLoss


AvgYield_Irrig	
         sdYield_Irrig	
         cvYield_Irrig	
         AvgYield_Temp	
         sdYield_Temp	
         cvYield_Temp	
         IT_ratio_AvgYield	
         IT_ratio_cvYield

         AvgYieldLoss_Irrig	
         sdYieldLoss_Irrig	
         cvYieldLoss_Irrig	
         AvgYieldLoss_Temp	
         sdYieldLoss_Temp	
         cvYieldLoss_Temp	
         IT_ratio_AvgYieldLoss	
         IT_ratio_cvYieldLoss

"AGType_Area_ha"        


"sdYield"              
 [11] "minYield"              "maxYield"              "medYield"              "cvYield"               "AvgPlanted"           
 [16] "AvgHarvested"          "sdHarvested"           "cvHarvested"           "AvgLost"               "sdLost"               
 [21] "cvLost"                "AvgProduct"            "AvgValue"              "AvgPrice"              "n"                    
 [26] "n_Total"               "AvgYield_Total"        "sdYield_Total"         "cvYield_Total"         "AvgYield_Irrig"       
 [31] "sdYield_Irrig"         "cvYield_Irrig"         "AvgYield_Temp"         "sdYield_Temp"          "cvYield_Temp"         
 [36] "IT_ratio_AvgYield"     "IT_ratio_cvYield"      
 
 
 "Area_ha"               "Pop_Total"             "Pop_Rural"            
 [41] "Pop_Urban"             "UrbRatio"              "Pop_Pct_Rural"         "Pop_Employed"          "Primary"              
 [46]            "AG_Workers"            "Pct_AG_Workers"        "Total_PU"              "Total_ha"             
 [51] "Primary_PU"            "Primary_ha"            "AG_ha"                 "Pasture_ha"            "CommonLands_ha"       
 [56] "Private_ha"            "Owned_ha"              "Rented_ha"             "Pct_Rented_ha"         "Rest_ha"              
 [61]            "Fallow_ha"                      "Riego_PU"Temporal_PU              "Reigo_ha"             
 [66] ""           "Temporal_ha"              "TotalLabor"            "TotalLabor_perHa"      "PctContractLabor"      "ContractLabor_Tot"    
 [81] "ContractLabor_More6Mo" "ContractLabor_Less6Mo" "FamLabor_Tot"          "MechanizEquip"         "Tractors"             
 [86] "Threshers"             "MotorizedCranes"       "OtherMachinery"        "DraftAnimals_Total"    "DraftAnimals_Oxen"    
 [91] "DraftAnimals_Horses"   "DraftAnimals_Mules"    "DraftAnimals_Asses"    "PopDens"               "PopDensRural"         
 [96] "PrimaryEmployDens"     "TotalLaborDens"        "AGPasture_ha"          "PrimaryEmploy_perHa"   "AG_Workers_perHa"     
[101] "MechanizEquip_perHa"   "DraftAnimals_perHa"    "Pct_AG_ha"                     "LU_MechanizEquip"     
[106] "LU_DraftAnimals"       "LU_FamLabor"           "LU_ContractFull"       "LU_ContractPart"                    
[111] 


### CHINAMPAS SBOM

```{r, label='xxx', message=FALSE,warning=FALSE}

```





## Merge Raster Data with AG Data

```{r}

Train = Tot %>% select(-Estado_ID, -Municipio_ID) %>% 
  left_join(env_vals, by=c("EstadoMunicipio","Estado", "Municipio", "AGType")) %>%
  select(AGType, Estado, Municipio, EstadoMunicipio, AvgYield, sdYield, cvYield, AvgYieldLoss, sdYieldLoss, cvYieldLoss, Pop_Total, Pop_Urban, UrbRatio, Pct_Primary, Pct_Rented_ha, Pct_Rest_ha, Pct_Fallow_ha, Pct_FertilChem_ha, Pct_FertilManure_ha, Pct_ImprovSeed_ha, Pct_Herbicides_ha, Pct_Insecticides_ha, Pct_ControlledBurn_ha, TotalLabor, TotalLabor_perHa, PctContractLabor, Pct_Pasture_ha, LU_Total, LU_Total_perHa, mean.Slope, mean.Accum, mean.TRI, mean.TWI, mean.STI, mean.SPI, mean.BD, mean.S, mean.Z, mean.C, mean.SOC, mean.CEC, mean.OCD, mean.N, mean.BDRICM, mean.BDTICM, mean.prec_04, mean.prec_05, mean.prec_06, mean.prec_07, mean.prec_08, mean.prec_09, mean.prec_10, mean.prec_11, mean.pr_04, mean.pr_05, mean.pr_06, mean.pr_07, mean.pr_08, mean.pr_09, mean.pr_10, mean.pr_11, mean.tmean_04, mean.tmean_05, mean.tmean_06, mean.tmean_07, mean.tmean_08, mean.tmean_09, mean.tmean_10, mean.tmean_11, mean.tmin_04, mean.tmin_05, mean.tmin_06, mean.tmin_07, mean.tmin_08, mean.tmin_09, mean.tmin_10, mean.tmin_11, mean.clt_04, mean.clt_05, mean.clt_06, mean.clt_07, mean.clt_08, mean.clt_09, mean.clt_10, mean.clt_11, mean.rsds_04, mean.rsds_05, mean.rsds_06, mean.rsds_07, mean.rsds_08, mean.rsds_09, mean.rsds_10, mean.rsds_11)

Temp = Train %>% filter(AGType == "Temporal") %>% select(-AGType)
Irrig = Train %>% filter(AGType == "Riego") %>% select(-AGType)

```


```{r}



```






### PCA for Variable Selection

```{r, label='PCA for Agricultural Variable Selection', message=FALSE,warning=FALSE}

# sdYield, cvYield, sdHarvested, cvHarvested, sdLost                

pca_tot <- prcomp(Train[,-c(1:4)], center = TRUE, scale. = TRUE)
loadings_tot = as.data.frame(pca_tot$rotation, row.names = colnames(Train[,-c(1:4)]))

pca_temp <- prcomp(Temp[,-c(1:3)], center = TRUE, scale. = TRUE)
loadings_temp = as.data.frame(pca_temp$rotation, row.names = colnames(Temp[,-c(1:3)]))

pca_irrig <- prcomp(Irrig[,-c(1:3)], center = TRUE, scale. = TRUE)
loadings_irrig = as.data.frame(pca_irrig$rotation, row.names = colnames(Irrig[,-c(1:3)]))

#res <- cor(y)
#res = as.data.frame(round(res, 2))

```


```{r}
hist(Temp$AvgYield, breaks = seq(0, 5200, by = 200), xlim = c(0,5200))
hist(Irrig$AvgYield, breaks = seq(2000, 10250, by = 250), xlim = c(2000,10250))
#rescale to 1500-8000
hist(Tot$AvgYield, breaks = seq(0, 10250, by = 250), xlim = c(0,10250))
hist(log(Irrig$AvgYield))
hist(log(Tot$AvgYield))

plot(Tot$LU_Total_perHa, Tot$AvgYield)
plot(log(Tot$LU_Total_perHa), log(Tot$AvgYield))
plot(Irrig$LU_Total_perHa, Irrig$AvgYield)
plot(log(Irrig$LU_Total_perHa), (Irrig$AvgYield))
plot(Temp$LU_Total_perHa, Temp$AvgYield)
plot(log(Temp$LU_Total_perHa), log(Temp$AvgYield))
```



### Subset Agricultural Variables

```{r, label='Subset Agricultural Variables', message=FALSE,warning=FALSE}

x <- Data2000s_poly %>% select(EstadoMunicipio, AvgYield, AGType, LU_Total_perHa, Pct_Primary, Pct_FertilChem_ha, Pct_FertilManure_ha, UrbRatio, Pct_AG_ha, TotalLaborDens, Pop_Pct_Rural, TotalLabor_perHa, Pct_Pasture_ha, geom)

```






# High Elevations


```{r, label='High Elevations', message=FALSE,warning=FALSE}

DEM <- raster(paste0(wd$data_r,"DEM_r.tif"))

#high_elev <- rast(high_elev)

high_elev[high_elev < 3100] <- NA

high_elev[high_elev >= 3100] <- 1

#### EDIT FROM HERE ####


high_elev <- rasterToPoints(high_elev,spatial=TRUE)

rand <- sample(nrow(high_elev), nrow(points)/4, replace=FALSE)

high_elev <- high_elev[rand,]
high_elev$Irr <- sample(0:1,nrow(high_elev),replace=TRUE)
set.seed(54865132)
high_elev$Fert <- sample(0:1,nrow(high_elev),replace=TRUE)
high_elev$MUNICIPIO <- "HIGH"
high_elev$X <- high_elev@coords[,'x']
high_elev$Y <- high_elev@coords[,'y']
high_elev$Suit <- 0
high_elev$Maize <- 0
high_elev <- high_elev[,which(names(high_elev) != "DEM")]
```



# Random Forest Model

When using a Random Forest model, there are several approaches you can employ to narrow down the variables used as input. These methods can help improve the model's performance, reduce overfitting, and enhance interpretability. Here are some commonly used approaches:

Feature Importance: Random Forest models provide a measure of feature importance, which indicates the relative contribution of each variable in predicting the target variable. You can use these importance scores to identify the most influential variables and select only the top-ranked ones for your model.

Recursive Feature Elimination (RFE): RFE is an iterative process that starts with all variables and progressively eliminates the least important ones. It trains the model on a subset of features, calculates their importance, and removes the least important feature. The process is repeated until a desired number of features is reached or a specified stopping criterion is met.

Selecting a Threshold: You can set a threshold for feature importance and select variables that exceed that threshold. This approach allows you to include only the most relevant variables in your model, discarding those with lower importance.

Correlation Analysis: Identify variables that have a high correlation with the target variable and retain those while eliminating highly correlated variables among themselves. This helps reduce redundancy and focuses on variables that have a stronger relationship with the target.

Domain Knowledge and Expert Opinion: Leverage your domain knowledge or consult subject matter experts to identify variables that are most likely to impact the target variable. This approach can help you narrow down the variables and focus on those that are known to be relevant in the specific context.

Stepwise Selection: Implement a stepwise selection algorithm, such as forward selection, backward elimination, or a combination of both. These methods iteratively add or remove variables based on statistical metrics like p-values, AIC, or BIC, to find the optimal subset of variables for your model.

L1 Regularization (LASSO): Apply L1 regularization, such as the LASSO (Least Absolute Shrinkage and Selection Operator), which can shrink the coefficients of irrelevant variables to zero, effectively excluding them from the model.

It is important to note that the selection of variables should be guided by the specific problem, the available data, and the goals of your analysis. Experimentation, validation, and careful consideration of the trade-offs between model complexity and performance are crucial in determining the final set of variables for your Random Forest model.
```{r}

train <- na.omit(train)


rf <- ranger(Maize ~ ., data = train[, !(names(train) %in% cols)]
, num.trees = 1000, write.forest = TRUE)


test_out <- predict(rf,test)
test$predicted <- test_out$predictions
test$error <- test$predicted - test$Maize

rsq <- 1 - (sd(test$error^2)/sd(test$Maize^2))
mse <- mean(test$error^2)
rmse <- sqrt(mse)
mae <- mean(abs(test$error))

{print(paste("Rsq:", rsq))
print(paste("RMSE:", rmse))
print(paste("MAE:", mae))}


null_irrigation <- all[["Slope"]]
null_irrigation[null_irrigation == null_irrigation] <- 0
null_fert <- null_irrigation
nulls <- stack(null_irrigation,null_fert)
names(nulls) <- c('Irr','Fert')


names(all) <- str_replace(names(all),"\\..*","")


n <- c(names(nulls),names(all))
n <- str_replace(names(n),"\\..*","")

predictors <- stack(nulls,all)

aoi <- extent(2710000,2950000,780000,950000)

predictors <- crop(predictors,aoi)
for (i in 1:nlayers(predictors)){
predictors[[i]] <- na.roughfix(predictors[[i]])
}

predictors <-approxNA(predictors)

output <- predict(rf,as.matrix(predictors),type="response")

new <- predictors[[1]]
new[1:length(new)] <- output$predictions


steep <- predictors[["Slope"]]
new[steep >= 7] <- 0
new[new > 2000] <- 2000

plot(new)

lake <- readOGR(dsn = "A:/Regional_Datasets/GIS/Lake_Texcoco",
layer = "Lake_Texcoco")
lake <- spTransform(lake,crs(new))
plot(lake,add=TRUE,col="cyan")

```





