---
title: "Aztec Agricultural Productivity Model"
subtitle: "Part 3: Random Forest Model"
author: "Rudolf Cesaretti"
date: "Last run on `r Sys.Date()`"
output:
  html_document:
    toc: true
    df_print: paged
    number_sections: true
bibliography: References.bib
csl: apa.csl
link-citations: yes
---

```{css, echo=FALSE}
pre {
  max-height: 300px;
  overflow-y: auto;
}

pre[class] {
  max-height: 300px;
}
```

```{r, setup, include=FALSE,echo=FALSE, message=FALSE,warning=FALSE}
require(knitr)
# Set so that long lines in R will be wrapped:
opts_chunk$set(tidy.opts=list(width.cutoff=75),tidy=TRUE)
#
rm(list = ls())
```

I do four things in this R markdown document: 
Topographic/environmental metrics

  1. Calculate catchment area variables, including
      + Topographic/environmental metrics
      + 
  4. Reorganize the data and export for Script #6
  
  
# Setup 

All of the data and scripts are downloadable from the [new ASU SettlementPersist2022 github repository](https://https://github.com/rcesaret/ASUSettlementPersist2022), which can be downloaded locally as a .zip folder or cloned to your own account.

Either way, once you have done so, you will need to modify the working directory (setwd("C:/...)") path and "dir" variables in the code chunk below to match the repository location on your computer.

```{r, label='Set Local Directory Location', message=FALSE,warning=FALSE}

wd <- list()

#SET YOUR LOCAL DIRECTORY LOCATION HERE:
wd$dir <- "D:/Dropbox (ASU)/AztecAgricultureModel/AztecAgriculture/"
#wd$dir <- "C:/Users/TJ McMote/Dropbox (ASU)/AztecAgricultureModel/AztecAgriculture/"

wd$analysis <- paste0(wd$dir,"analysis/")
wd$data_r <- paste0(wd$dir,"data-raw/")
wd$data_p <- paste0(wd$dir,"data-processed/")
wd$data_f <- paste0(wd$dir,"data-final-outputs/")
wd$figs <- paste0(wd$dir,"figures/")
wd$funcs <- paste0(wd$dir,"functions/")

```



## Load R Packages and Custom Functions

```{r, label='Load Libraries', message=FALSE,warning=FALSE}
# Package names
packages <- c("tidyverse", "rgdal", "rgeos", "sp", "sf", "GISTools", "raster", 
              "Matrix", "terra","gdistance", "lwgeom", "tidyr", "stars", 
              "dismo", "purrr", "spatialEco", "whitebox", "classInt",
              "ggnewscale", "lbmech", "data.table", "tidyterra","gridExtra", 
              "cowplot", "scam", "rmarkdown", "spatialreg","spdep", "ggridges", 
              "ggnewscale", "scales", "ggstatsplot", "stringi", "fuzzyjoin", 
              "mgcv", "randomForest", "ranger", "exactextractr", "kableExtra",
              "Boruta", "mlbench", "caret")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# load packages
invisible(lapply(packages, library, character.only = TRUE))

rm(packages,installed_packages)

#Read in custom R functions located in the wd$funcs directory folder
FUNCS <- list("RescaleSpatRast.R", "Terra_df.R", "RF_ImputeRast.R", "linear_rescale.R")
invisible(lapply(FUNCS, function(x) source(paste0(wd$funcs,x))))
rm(FUNCS)

```




## Load Data from Previous Script

```{r, label='Import Data from Part 1', message=FALSE,warning=FALSE}
Municipios2000s <- st_read(paste0(wd$data_p, "Municipios2000s_Data.gpkg"))

Data2000s_poly <- st_read(paste0(wd$data_p, "Data2000s_poly.gpkg"))

TopoEnv_r <- rast(paste0(wd$data_p, "TopoEnvData.tif"))

Soil_r <- rast(paste0(wd$data_p, "Soil_r_resampled.tif"))

Clim_r <- rast(paste0(wd$data_p, "Clim_r_resampled.tif"))

```


# High Elevations


```{r, label='High Elevations', message=FALSE,warning=FALSE}

DEM <- raster(paste0(wd$data_r,"DEM_r.tif"))

#high_elev <- rast(high_elev)

high_elev[high_elev < 3100] <- NA

high_elev[high_elev >= 3100] <- 1

#### EDIT FROM HERE ####


high_elev <- rasterToPoints(high_elev,spatial=TRUE)

rand <- sample(nrow(high_elev), nrow(points)/4, replace=FALSE)

high_elev <- high_elev[rand,]
high_elev$Irr <- sample(0:1,nrow(high_elev),replace=TRUE)
set.seed(54865132)
high_elev$Fert <- sample(0:1,nrow(high_elev),replace=TRUE)
high_elev$MUNICIPIO <- "HIGH"
high_elev$X <- high_elev@coords[,'x']
high_elev$Y <- high_elev@coords[,'y']
high_elev$Suit <- 0
high_elev$Maize <- 0
high_elev <- high_elev[,which(names(high_elev) != "DEM")]
```





# Construct Training Dataset

## Environmental Raster Data

### Subset Raster Data

```{r, label='Subset Environmental Data Rasters', message=FALSE,warning=FALSE}
Soil_r_sub <- subset(Soil_r, c("BD","S", "Z","C","SOC","CEC","OCD","N", "BDRICM", "BDTICM"))

TopoEnv_r_sub <- subset(TopoEnv_r, c("DEM", "Slope","Accum", "TRI", "TWI", "STI", "SPI"))

Clim_r_sub <- subset(Clim_r, c("prec_04", "prec_05", "prec_06", "prec_07", "prec_08", "prec_09", "prec_10", "prec_11",
                               "pr_04", "pr_05", "pr_06", "pr_07", "pr_08", "pr_09", "pr_10", "pr_11",
                               "tmean_04", "tmean_05", "tmean_06", "tmean_07", "tmean_08", "tmean_09", "tmean_10", "tmean_11", 
                               "tmin_04","tmin_05", "tmin_06", "tmin_07", "tmin_08", "tmin_09", "tmin_10","tmin_11",
                               "clt_04", "clt_05", "clt_06", "clt_07", "clt_08", "clt_09", "clt_10", "clt_11", 
                               "rsds_04", "rsds_05", "rsds_06", "rsds_07", "rsds_08", "rsds_09", "rsds_10", "rsds_11"
                               ))

```



### Extract Raster Values

```{r, label='Extract Raster Values', message=FALSE,warning=FALSE}

topo_vals <- exact_extract(x = TopoEnv_r_sub, y = Data2000s_poly, fun = "mean", 
                           max_cells_in_memory = 8e+08, stack_apply=T, 
                           append_cols = c("EstadoMunicipio","Estado", "Municipio", "AGType"))

soil_vals <- exact_extract(x = Soil_r_sub, y = Data2000s_poly, fun = "mean", 
                           max_cells_in_memory = 8e+08, stack_apply=T, 
                           append_cols = c("EstadoMunicipio","Estado", "Municipio", "AGType"))

clim_vals <- exact_extract(x = Clim_r_sub, y = Data2000s_poly, fun = "mean", 
                           max_cells_in_memory = 8e+08, stack_apply=T, 
                           append_cols = c("EstadoMunicipio","Estado", "Municipio", "AGType"))

env_vals <- topo_vals %>% left_join(soil_vals, by = c("EstadoMunicipio","Estado", "Municipio", "AGType")) %>% 
                          left_join(clim_vals, by = c("EstadoMunicipio","Estado", "Municipio", "AGType")) %>% 
                          mutate(AGType = as.factor(AGType))
  

#env_vals = merge(topo_vals, soil_vals, all = F, sort = F)

rm(topo_vals, soil_vals, clim_vals)

```

```{r}
#values2 <- exact_extract(x = Soil_r_sub, y = Data2000s_poly, fun = "count", max_cells_in_memory = 8e+08, stack_apply=T, append_cols = c("EstadoMunicipio","Estado", "Municipio", "AGType"))

#xx = cbind(st_drop_geometry(Data2000s_poly$EstadoMunicipio), values)

#values <- exact_extract(x = Soil_r_sub[[1:2]], y = Data2000s_poly, fun = mymean, summarize_df = TRUE, max_cells_in_memory = 8e+08, stack_apply=T)

all <- c(Clim_r_sub, Soil_r_sub, TopoEnv_r_sub)

Soil_df <- exact_extract(x = Soil_r_sub, y = Data2000s_poly, fun = mymean, max_cells_in_memory = 8e+10)
save(all_df, file = paste0(wd$data_r, "EnvRastPolyExtract_df.rData"))


st_extract(Clim_r[,,,5], x, fun = mean)

Clim_df <- data.frame()

for (i in 1:nlyr(Clim_r)) {
  out[, i] <- my_function(my_df[, i])
}

nlyr(Clim_r)
values <- exact_extract(x = Clim_r[[1:2]], y = x, fun = "mean", max_cells_in_memory = 8e+08)

c("prec_05", "prec_06", "prec_07", "prec_08", "prec_09", "prec_10", 
"pr_05", "pr_06", "pr_07", "pr_08", "pr_09", "pr_10", 
"tmean_05", "tmean_06", "tmean_07", "tmean_08", "tmean_09", "tmean_10", 
"tmin_05", "tmin_06", "tmin_07", "tmin_08", "tmin_09", "tmin_10")

clim <- stack(prec[[5:7]],prec[[5:7]],
prec[[8:10]],
tmin[[5:7]],
tmin[[8:10]],tmin[[8:10]])
geol <- stack(depth, cation, slope)

geol_vals <- extract(geol,points)
clim_vals <- extract(clim,points)
```




Rescaling/Normalizing the data is unnecessary!! 
Random Forest algorithm is not a distance-based model - it is a tree-based model. Each node in a Random Forest is not comparing feature values, it is simply splitting a sorted list that requires absolute values for branching. The algorithm is based on partitioning the data to make predictions, therefore, it does not require normalization.

https://www.kdnuggets.com/2022/07/random-forest-algorithm-need-normalization.html#:~:text=Each%20node%20in%20a%20Random,it%20does%20not%20require%20normalization.



## Agricultural Data


```{r, label='Prepare Agricultural Data', message=FALSE,warning=FALSE}

Data2000s_poly <- Data2000s_poly %>% mutate(
  AGType = as.factor(AGType),
  AvgYield = as.numeric(AvgYield),
  sdYield = as.numeric(sdYield),
  minYield = as.numeric(minYield),
  maxYield = as.numeric(maxYield),
  medYield = as.numeric(medYield),
  cvYield = as.numeric(cvYield),
  
  AvgYieldLoss = as.numeric(AvgYieldLoss),
  sdYieldLoss = as.numeric(sdYieldLoss),
  minYieldLoss = as.numeric(minYieldLoss),
  maxYieldLoss = as.numeric(maxYieldLoss),
  medYieldLoss = as.numeric(medYieldLoss),
  cvYieldLoss = as.numeric(cvYieldLoss),
  
  AvgPlanted = as.numeric(AvgPlanted),
  AvgHarvested = as.numeric(AvgHarvested),
  sdHarvested = as.numeric(sdHarvested),
  cvHarvested = as.numeric(cvHarvested),
  AvgLost = as.numeric(AvgLost),
  sdLost = as.numeric(sdLost),
  cvLost = as.numeric(cvLost),
  AvgProduct = as.numeric(AvgProduct),
  AvgValue = as.numeric(AvgValue),
  AvgPrice = as.numeric(AvgPrice),
  n = as.numeric(n),
  n_Total = as.numeric(n_Total),
  AvgYield_Total = as.numeric(AvgYield_Total),
  sdYield_Total = as.numeric(sdYield_Total),
  cvYield_Total = as.numeric(cvYield_Total),
  AvgYield_Irrig = as.numeric(AvgYield_Irrig),
  sdYield_Irrig = as.numeric(sdYield_Irrig),
  cvYield_Irrig = as.numeric(cvYield_Irrig),
  AvgYield_Temp = as.numeric(AvgYield_Temp),
  sdYield_Temp = as.numeric(sdYield_Temp),
  cvYield_Temp = as.numeric(cvYield_Temp),
  IT_ratio_AvgYield = as.numeric(IT_ratio_AvgYield),
  IT_ratio_cvYield = as.numeric(IT_ratio_cvYield),
  
  AvgYieldLoss_Total = as.numeric(AvgYieldLoss_Total),
  sdYieldLoss_Total = as.numeric(sdYieldLoss_Total),
  cvYieldLoss_Total = as.numeric(cvYieldLoss_Total),
  AvgYieldLoss_Irrig = as.numeric(AvgYieldLoss_Irrig),
  sdYieldLoss_Irrig = as.numeric(sdYieldLoss_Irrig),
  cvYieldLoss_Irrig = as.numeric(cvYieldLoss_Irrig),
  AvgYieldLoss_Temp = as.numeric(AvgYieldLoss_Temp),
  sdYieldLoss_Temp = as.numeric(sdYieldLoss_Temp),
  cvYieldLoss_Temp = as.numeric(cvYieldLoss_Temp),
  IT_ratio_AvgYieldLoss = as.numeric(IT_ratio_AvgYieldLoss),
  IT_ratio_cvYieldLoss = as.numeric(IT_ratio_cvYieldLoss),
  
  Pct_Tract_Mechaniz_pu = as.numeric(Pct_Tract_Mechaniz_pu),
  Pct_Tract_Animal_pu = as.numeric(Pct_Tract_Animal_pu),
  Pct_Tract_Manual_pu = as.numeric(Pct_Tract_Manual_pu),
  Riego_PU = as.numeric(Riego_PU),
  Temporal_PU = as.numeric(Temporal_PU)
  )

```


```{r, label='Prepare Agricultural Data', message=FALSE,warning=FALSE}

x = st_drop_geometry(Data2000s_poly)# %>% filter(-1:3)
x = x %>% filter(!(EstadoMunicipio %in% c("Ixtapaluca, Mexico", "Valle de Chalco Solidaridad, Mexico", "Milpa Alta, Distrito Federal", "Tlahuac, Distrito Federal", "Xochimilco, Distrito Federal", "Axapusco, Mexico") & AGType %in% "Riego"))

#x = x[,-c(1:5)]
#x = as.data.frame(x)
Tot = x %>% select(-AvgYield_Total, -sdYield_Total, -cvYield_Total, 
                   -AvgYield_Irrig, -sdYield_Irrig, -cvYield_Irrig, -AvgYield_Temp, 
                   -sdYield_Temp, -cvYield_Temp, -IT_ratio_AvgYield, -IT_ratio_cvYield, 
                   -AvgYieldLoss_Total, -sdYieldLoss_Total, -cvYieldLoss_Total, 
                   -AvgYieldLoss_Irrig, -sdYieldLoss_Irrig, -cvYieldLoss_Irrig, 
                   -AvgYieldLoss_Temp, -sdYieldLoss_Temp, -cvYieldLoss_Temp, 
                   -IT_ratio_AvgYieldLoss,  -IT_ratio_cvYieldLoss, -minYieldLoss) %>% 
  filter(complete.cases(.))


# Find rows with NA, NaN, or Inf values
#rows_with_na <- apply(Tot, 1, function(row) any(is.na(row) | is.nan(row) | is.infinite(row)))

# Find columns with NA, NaN, or Inf values
#columns_with_na <- apply(Tot, 2, function(column) any(is.na(column) | is.nan(column) | is.infinite(column)))

# Print the rows and columns with NA, NaN, or Inf values
#cat("Rows with NA, NaN, or Inf values:\n")
#print(which(rows_with_na))
#cat("\n")
#cat("Columns with NA, NaN, or Inf values:\n")
#print(which(columns_with_na))

```
--n = 1 case--
***- Riego
Malinalco, Mexico 
Chicoloapan, Mexico
Chimalhuacan, Mexico

sdYield     cvYield sdYieldLoss cvYieldLoss sdHarvested cvHarvested      sdLost      cvLost 


***- Riego
Ixtapaluca, Mexico 
Valle de Chalco Solidaridad, Mexico
Milpa Alta, Distrito Federal
Tlahuac, Distrito Federal
Xochimilco, Distrito Federal
Axapusco, Mexico




-AvgYieldLoss_Irrig, -sdYieldLoss_Irrig, -cvYieldLoss_Irrig, -AvgYieldLoss_Temp, -sdYieldLoss_Temp, -cvYieldLoss_Temp, -IT_ratio_AvgYieldLoss,  -IT_ratio_cvYieldLoss


AvgYield_Irrig	
         sdYield_Irrig	
         cvYield_Irrig	
         AvgYield_Temp	
         sdYield_Temp	
         cvYield_Temp	
         IT_ratio_AvgYield	
         IT_ratio_cvYield

         AvgYieldLoss_Irrig	
         sdYieldLoss_Irrig	
         cvYieldLoss_Irrig	
         AvgYieldLoss_Temp	
         sdYieldLoss_Temp	
         cvYieldLoss_Temp	
         IT_ratio_AvgYieldLoss	
         IT_ratio_cvYieldLoss






## Merge Raster Data with AG Data

```{r}

Train_Total = Tot %>% select(-Estado_ID, -Municipio_ID) %>% 
  left_join(env_vals, by=c("EstadoMunicipio","Estado", "Municipio", "AGType")) %>%
  select(AGType, AvgYield, sdYield, cvYield, AvgYieldLoss, sdYieldLoss, cvYieldLoss, Pop_Total, Pop_Urban, UrbRatio, Pct_Primary, Pct_Rented_ha, Pct_Rest_ha, Pct_Fallow_ha, Pct_FertilChem_ha, Pct_FertilManure_ha, Pct_ImprovSeed_ha, Pct_Herbicides_ha, Pct_Insecticides_ha, Pct_ControlledBurn_ha, TotalLabor, TotalLabor_perHa, PctContractLabor, Pct_Pasture_ha, LU_Total, LU_Total_perHa, mean.DEM, mean.Slope, mean.Accum, mean.TRI, mean.TWI, mean.STI, mean.SPI, mean.BD, mean.S, mean.Z, mean.C, mean.SOC, mean.CEC, mean.OCD, mean.N, mean.BDRICM, mean.BDTICM, mean.prec_04, mean.prec_05, mean.prec_06, mean.prec_07, mean.prec_08, mean.prec_09, mean.prec_10, mean.prec_11, mean.pr_04, mean.pr_05, mean.pr_06, mean.pr_07, mean.pr_08, mean.pr_09, mean.pr_10, mean.pr_11, mean.tmean_04, mean.tmean_05, mean.tmean_06, mean.tmean_07, mean.tmean_08, mean.tmean_09, mean.tmean_10, mean.tmean_11, mean.tmin_04, mean.tmin_05, mean.tmin_06, mean.tmin_07, mean.tmin_08, mean.tmin_09, mean.tmin_10, mean.tmin_11, mean.clt_04, mean.clt_05, mean.clt_06, mean.clt_07, mean.clt_08, mean.clt_09, mean.clt_10, mean.clt_11, mean.rsds_04, mean.rsds_05, mean.rsds_06, mean.rsds_07, mean.rsds_08, mean.rsds_09, mean.rsds_10, mean.rsds_11) %>%
  mutate(AGType = as.factor(AGType))

Train_Temp = Train %>% filter(AGType == "Temporal") %>% select(-AGType)

Train_Irrig = Train %>% filter(AGType == "Riego") %>% select(-AGType)

Train_Env = Tot %>% select(-Estado_ID, -Municipio_ID) %>% 
  left_join(env_vals, by=c("EstadoMunicipio","Estado", "Municipio", "AGType")) %>%
  select(AGType, mean.DEM, mean.Slope, mean.Accum, mean.TRI, mean.TWI, mean.STI, mean.SPI, mean.BD, mean.S, mean.Z, mean.C, mean.SOC, mean.CEC, mean.OCD, mean.N, mean.BDRICM, mean.BDTICM, mean.prec_04, mean.prec_05, mean.prec_06, mean.prec_07, mean.prec_08, mean.prec_09, mean.prec_10, mean.prec_11, mean.pr_04, mean.pr_05, mean.pr_06, mean.pr_07, mean.pr_08, mean.pr_09, mean.pr_10, mean.pr_11, mean.tmean_04, mean.tmean_05, mean.tmean_06, mean.tmean_07, mean.tmean_08, mean.tmean_09, mean.tmean_10, mean.tmean_11, mean.tmin_04, mean.tmin_05, mean.tmin_06, mean.tmin_07, mean.tmin_08, mean.tmin_09, mean.tmin_10, mean.tmin_11, mean.clt_04, mean.clt_05, mean.clt_06, mean.clt_07, mean.clt_08, mean.clt_09, mean.clt_10, mean.clt_11, mean.rsds_04, mean.rsds_05, mean.rsds_06, mean.rsds_07, mean.rsds_08, mean.rsds_09, mean.rsds_10, mean.rsds_11) %>%
  mutate(AGType = as.factor(AGType))


```





### PCA for Variable Selection

```{r, label='PCA for Agricultural Variable Selection', message=FALSE,warning=FALSE}

# sdYield, cvYield, sdHarvested, cvHarvested, sdLost                

pca_tot <- prcomp(Train[,-c(1:4)], center = TRUE, scale. = TRUE)
loadings_tot = as.data.frame(pca_tot$rotation, row.names = colnames(Train[,-c(1:4)]))

pca_temp <- prcomp(Temp[,-c(1:3)], center = TRUE, scale. = TRUE)
loadings_temp = as.data.frame(pca_temp$rotation, row.names = colnames(Temp[,-c(1:3)]))

pca_irrig <- prcomp(Irrig[,-c(1:3)], center = TRUE, scale. = TRUE)
loadings_irrig = as.data.frame(pca_irrig$rotation, row.names = colnames(Irrig[,-c(1:3)]))

#res <- cor(y)
#res = as.data.frame(round(res, 2))

```


```{r}
hist(Temp$AvgYield, breaks = seq(0, 5200, by = 200), xlim = c(0,5200))
hist(Irrig$AvgYield, breaks = seq(2000, 10250, by = 250), xlim = c(2000,10250))
#rescale to 1500-8000
hist(Tot$AvgYield, breaks = seq(0, 10250, by = 250), xlim = c(0,10250))
hist(log(Irrig$AvgYield))
hist(log(Tot$AvgYield))

plot(Tot$LU_Total_perHa, Tot$AvgYield)
plot(log(Tot$LU_Total_perHa), log(Tot$AvgYield))
plot(Irrig$LU_Total_perHa, Irrig$AvgYield)
plot(log(Irrig$LU_Total_perHa), (Irrig$AvgYield))
plot(Temp$LU_Total_perHa, Temp$AvgYield)
plot(log(Temp$LU_Total_perHa), log(Temp$AvgYield))
```

### Boruta for Variable Selection

Random Forest feature selection, why we need feature selection?

When we have too many features in the datasets and we want to develop a prediction model like a neural network will take a lot of time and reduces the accuracy of the prediction model.

We need to make use of the Boruta algorithm and is based on random forest.

How Boruta works?
Suppose if you have 100 variables in the dataset, each attributes creates shadow attributes, and in each shadow attribute, all the values are shuffled and creates randomness in the dataset.

Based on these datasets will create a classification model with shadow attributes and original attributes and then assess the importance of the attributes.
https://www.r-bloggers.com/2021/05/random-forest-feature-selection/
Boruta is an all relevant feature selection wrapper algorithm, capable of working with any classification method that output variable importance measure (VIM); by default, Boruta uses Random
Forest. The method performs a top-down search for relevant features by comparing original attributes’ importance with importance achievable at random, estimated using their permuted copies,
and progressively eliminating irrelevant features to stabilise that test.
https://cran.r-project.org/web/packages/Boruta/Boruta.pdf

```{r}
#Response = Train$AvgYield
#Predictors = Train %>% select(-EstadoMunicipio, -Estado, -Municipio, -AvgYield)
TrainB = Train %>% select(-EstadoMunicipio, -Estado, -Municipio)

TrainB2 = Tot %>% select(-Estado_ID, -Municipio_ID) %>% 
  left_join(env_vals, by=c("EstadoMunicipio","Estado", "Municipio", "AGType")) %>%
  select(-EstadoMunicipio, -Estado, -Municipio) %>%
  select(AGType, AvgYield, cvYield, AvgYieldLoss, cvYieldLoss, Pop_Total, Pop_Rural, Pop_Urban, UrbRatio, Pop_Pct_Rural, Primary, Pct_Primary, AG_Workers, Pct_AG_Workers, Pct_Rented_ha, Pct_Rest_ha, Pct_Fallow_ha, Pct_FertilChem_ha, Pct_FertilManure_ha, Pct_ImprovSeed_ha, Pct_Herbicides_ha, Pct_Insecticides_ha, Pct_ControlledBurn_ha, TotalLabor, TotalLabor_perHa, PrimaryEmploy_perHa, AG_Workers_perHa, PctContractLabor, LU_Total, LU_Total_perHa, PopDensRural, PopDens, mean.DEM, mean.Slope, mean.Accum, mean.TRI, mean.TWI, mean.STI, mean.SPI, mean.BD, mean.S, mean.Z, mean.C, mean.SOC, mean.CEC, mean.OCD, mean.N, mean.BDRICM, mean.BDTICM, mean.prec_04, mean.prec_05, mean.prec_06, mean.prec_07, mean.prec_08, mean.prec_09, mean.prec_10, mean.prec_11, mean.pr_04, mean.pr_05, mean.pr_06, mean.pr_07, mean.pr_08, mean.pr_09, mean.pr_10, mean.pr_11, mean.tmean_04, mean.tmean_05, mean.tmean_06, mean.tmean_07, mean.tmean_08, mean.tmean_09, mean.tmean_10, mean.tmean_11, mean.tmin_04, mean.tmin_05, mean.tmin_06, mean.tmin_07, mean.tmin_08, mean.tmin_09, mean.tmin_10, mean.tmin_11, mean.clt_04, mean.clt_05, mean.clt_06, mean.clt_07, mean.clt_08, mean.clt_09, mean.clt_10, mean.clt_11, mean.rsds_04, mean.rsds_05, mean.rsds_06, mean.rsds_07, mean.rsds_08, mean.rsds_09, mean.rsds_10, mean.rsds_11) %>% 
  mutate(Fertilizer = Pct_FertilManure_ha + Pct_FertilChem_ha) %>% 
  mutate(AGType = as.factor(AGType))

boruta <- Boruta(AvgYield ~ ., data = TrainB, maxRuns = 1000)#, doTrace = 2
boruta2 <- Boruta(AvgYield ~ ., data = TrainB2, maxRuns = 1000)#, doTrace = 2
print(boruta)
boruta[["finalDecision"]]
plot(boruta, las = 2, cex.axis = 0.7)
plot(boruta2, las = 2, cex.axis = 0.7)

bor <- TentativeRoughFix(boruta)
bor[["finalDecision"]]
bor2 <- TentativeRoughFix(boruta2)
bor2[["finalDecision"]]

b=attStats(boruta)
b2=attStats(boruta2)

form=getConfirmedFormula(boruta)
```
Pop_Total   
Pop_Urban 
Primary                                
Primary                                Rejected
Pct_Primary                            Rejected
AG_Workers                             Rejected
Pct_AG_Workers                         Rejected
Pct_Rented_ha                          Rejected
Pct_Rest_ha                            Rejected
Pct_Fallow_ha                          Rejected
Pct_FertilManure_ha                    Rejected
Pct_ImprovSeed_ha                      Rejected
Pct_Insecticides_ha                    Rejected
Pct_ControlledBurn_ha                  Rejected
Pct_Tract_Mechaniz_pu                  Rejected
Pct_Tract_Manual_pu                    Rejected
Pct_Pasture_ha                         Rejected
LU_FamLabor                            Rejected
LU_MechanizEquip                       Rejected
LU_DraftAnimals                        Rejected
PopDensRural                           Rejected
Temporal_PU                            Rejected
PopDensRural 

### Subset Agricultural Variables

```{r, label='Subset Agricultural Variables', message=FALSE,warning=FALSE}

Train = Tot %>% select(-Estado_ID, -Municipio_ID) %>% 
  left_join(env_vals, by=c("EstadoMunicipio","Estado", "Municipio", "AGType")) %>%
  select(AGType, Estado, Municipio, EstadoMunicipio, AvgYield, sdYield, cvYield, AvgYieldLoss, sdYieldLoss, cvYieldLoss, Pop_Total, Pop_Urban, UrbRatio, Pct_Primary, Pct_Rented_ha, Pct_Rest_ha, Pct_Fallow_ha, Pct_FertilChem_ha, Pct_FertilManure_ha, Pct_ImprovSeed_ha, Pct_Herbicides_ha, Pct_Insecticides_ha, Pct_ControlledBurn_ha, TotalLabor, TotalLabor_perHa, PctContractLabor, Pct_Pasture_ha, LU_Total, LU_Total_perHa, mean.Slope, mean.Accum, mean.TRI, mean.TWI, mean.STI, mean.SPI, mean.BD, mean.S, mean.Z, mean.C, mean.SOC, mean.CEC, mean.OCD, mean.N, mean.BDRICM, mean.BDTICM, mean.prec_04, mean.prec_05, mean.prec_06, mean.prec_07, mean.prec_08, mean.prec_09, mean.prec_10, mean.prec_11, mean.pr_04, mean.pr_05, mean.pr_06, mean.pr_07, mean.pr_08, mean.pr_09, mean.pr_10, mean.pr_11, mean.tmean_04, mean.tmean_05, mean.tmean_06, mean.tmean_07, mean.tmean_08, mean.tmean_09, mean.tmean_10, mean.tmean_11, mean.tmin_04, mean.tmin_05, mean.tmin_06, mean.tmin_07, mean.tmin_08, mean.tmin_09, mean.tmin_10, mean.tmin_11, mean.clt_04, mean.clt_05, mean.clt_06, mean.clt_07, mean.clt_08, mean.clt_09, mean.clt_10, mean.clt_11, mean.rsds_04, mean.rsds_05, mean.rsds_06, mean.rsds_07, mean.rsds_08, mean.rsds_09, mean.rsds_10, mean.rsds_11)%>%
  mutate(AGType = as.factor(AGType))

Temp = Train %>% filter(AGType == "Temporal") %>% select(-AGType)
Irrig = Train %>% filter(AGType == "Riego") %>% select(-AGType)

```






# Random Forest Model

When using a Random Forest model, there are several approaches you can employ to narrow down the variables used as input. These methods can help improve the model's performance, reduce overfitting, and enhance interpretability. Here are some commonly used approaches:

Feature Importance: Random Forest models provide a measure of feature importance, which indicates the relative contribution of each variable in predicting the target variable. You can use these importance scores to identify the most influential variables and select only the top-ranked ones for your model.

Recursive Feature Elimination (RFE): RFE is an iterative process that starts with all variables and progressively eliminates the least important ones. It trains the model on a subset of features, calculates their importance, and removes the least important feature. The process is repeated until a desired number of features is reached or a specified stopping criterion is met.

Selecting a Threshold: You can set a threshold for feature importance and select variables that exceed that threshold. This approach allows you to include only the most relevant variables in your model, discarding those with lower importance.

Correlation Analysis: Identify variables that have a high correlation with the target variable and retain those while eliminating highly correlated variables among themselves. This helps reduce redundancy and focuses on variables that have a stronger relationship with the target.

Domain Knowledge and Expert Opinion: Leverage your domain knowledge or consult subject matter experts to identify variables that are most likely to impact the target variable. This approach can help you narrow down the variables and focus on those that are known to be relevant in the specific context.

Stepwise Selection: Implement a stepwise selection algorithm, such as forward selection, backward elimination, or a combination of both. These methods iteratively add or remove variables based on statistical metrics like p-values, AIC, or BIC, to find the optimal subset of variables for your model.

L1 Regularization (LASSO): Apply L1 regularization, such as the LASSO (Least Absolute Shrinkage and Selection Operator), which can shrink the coefficients of irrelevant variables to zero, effectively excluding them from the model.

It is important to note that the selection of variables should be guided by the specific problem, the available data, and the goals of your analysis. Experimentation, validation, and careful consideration of the trade-offs between model complexity and performance are crucial in determining the final set of variables for your Random Forest model.
```{r}

train <- na.omit(train)


rf_yield <- ranger(formula=form, data = Train, num.trees = 1000, importance='impurity', write.forest = TRUE)

print(m.lzn.ra)

test_out <- predict(rf,test)
test$predicted <- test_out$predictions
test$error <- test$predicted - test$Maize

rsq <- 1 - (sd(test$error^2)/sd(test$Maize^2))
mse <- mean(test$error^2)
rmse <- sqrt(mse)
mae <- mean(abs(test$error))

{print(paste("Rsq:", rsq))
print(paste("RMSE:", rmse))
print(paste("MAE:", mae))}


null_irrigation <- all[["Slope"]]
null_irrigation[null_irrigation == null_irrigation] <- 0
null_fert <- null_irrigation
nulls <- stack(null_irrigation,null_fert)
names(nulls) <- c('Irr','Fert')


names(all) <- str_replace(names(all),"\\..*","")


n <- c(names(nulls),names(all))
n <- str_replace(names(n),"\\..*","")

predictors <- stack(nulls,all)

aoi <- extent(2710000,2950000,780000,950000)

predictors <- crop(predictors,aoi)
for (i in 1:nlayers(predictors)){
predictors[[i]] <- na.roughfix(predictors[[i]])
}

predictors <-approxNA(predictors)

output <- predict(rf,as.matrix(predictors),type="response")

new <- predictors[[1]]
new[1:length(new)] <- output$predictions


steep <- predictors[["Slope"]]
new[steep >= 7] <- 0
new[new > 2000] <- 2000

plot(new)

lake <- readOGR(dsn = "A:/Regional_Datasets/GIS/Lake_Texcoco",
layer = "Lake_Texcoco")
lake <- spTransform(lake,crs(new))
plot(lake,add=TRUE,col="cyan")

```





